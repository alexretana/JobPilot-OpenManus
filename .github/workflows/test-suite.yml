name: ðŸ§ª JobPilot-OpenManus CI/CD Pipeline

on:
  push:
    branches: [ main, develop, 'feature/**' ]
  pull_request:
    branches: [ main, develop ]

env:
  PYTHON_VERSION: '3.12'
  NODE_VERSION: '18'
  # Test configuration
  TEST_MODE: true
  PYTEST_CURRENT_TEST: true
  DATABASE_URL: sqlite:///./test_jobpilot.db
  CI: true
  PYTHONUNBUFFERED: 1
  PYTHONDONTWRITEBYTECODE: 1
  PIP_DISABLE_PIP_VERSION_CHECK: 1

jobs:
  # Quick validation - runs first for fast feedback
  quick-validation:
    name: âš¡ Quick Validation
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4

    - name: ðŸ Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: ðŸ“¦ Install basic dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-ci.txt

    - name: ðŸ” Python syntax validation
      run: |
        echo "ðŸ”§ Checking Python syntax..."
        python -m py_compile app/api/user_profiles.py
        python -m py_compile app/data/database.py
        python -m py_compile app/data/models.py
        echo "âœ… Syntax check passed"

    - name: ðŸ§ª Quick model validation
      run: |
        echo "ðŸ—ï¸ Running quick model validation..."
        python -c "
        from app.data.models import UserProfile, JobType, RemoteType
        from app.api.user_profiles import UserProfileCreate, UserProfileResponse
        print('âœ… User Profiles models imported successfully')
        print('âœ… API models validated')
        "

    - name: ðŸ—„ï¸ Database schema validation
      run: |
        echo "ðŸ—„ï¸ Validating database schema and models..."
        python -c "
        from app.data.models import Base, UserProfileDB, JobListingDB, JobApplicationDB
        from app.data.models import create_database_engine, create_tables
        engine = create_database_engine('sqlite:///test_quick_validation.db')
        create_tables(engine)
        print('âœ… Database schema validation passed')
        "

    - name: ðŸ”Œ API endpoint structure validation
      run: |
        echo "ðŸŒ Validating API endpoints structure..."
        python -c "
        from app.api.user_profiles import router
        print(f'âœ… User Profiles API routes: {len(router.routes)} endpoints')
        for route in router.routes:
            if hasattr(route, 'methods') and hasattr(route, 'path'):
                methods = ', '.join(route.methods) if route.methods else 'N/A'
                print(f'  {methods}: {route.path}')
        "

  # Backend API Tests - runs in parallel with quick validation
  backend-tests:
    name: ðŸš€ Backend API Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: quick-validation

    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4

    - name: ðŸ Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: ðŸ“¦ Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements-ci.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: ðŸ“¦ Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-ci.txt
        pip install pytest pytest-cov pytest-html pytest-xdist pytest-xvfb

    - name: ðŸ¥ Health check - verify installation
      run: |
        python --version
        pip list | grep -E "(fastapi|pydantic|sqlalchemy|pytest)"

    - name: ðŸ§ª Run comprehensive backend tests
      run: |
        echo "ðŸ”§ Running comprehensive user profiles database tests..."
        python run_tests.py --backend
        echo "ðŸŒ Running CI-friendly FastAPI backend tests..."
        pytest tests/backend/api/test_backend_fastapi_ci.py -v --tb=short --disable-warnings
        pytest tests/backend/test_user_profiles_pytest.py -v --tb=short --disable-warnings

    - name: ðŸ§ª Run core component tests
      run: |
        echo "ðŸ—ï¸ Running core component tests..."
        pytest test_core_components.py -v --tb=short --disable-warnings

    - name: ðŸ§ª Run additional backend validation tests
      run: |
        echo "ðŸ“Š Running additional backend validation tests..."
        # Run any additional test files in the root directory
        for test_file in test_*.py; do
          if [[ -f "$test_file" && "$test_file" != "test_user_profiles.py" && "$test_file" != "test_core_components.py" ]]; then
            echo "Running $test_file..."
            python "$test_file" || echo "âš ï¸ $test_file failed or requires special setup"
          fi
        done

    - name: ðŸ“Š Generate test summary
      run: |
        echo "ðŸ“Š Backend Test Summary:" > backend_test_summary.txt
        echo "===================" >> backend_test_summary.txt
        echo "âœ… User Profiles Database Tests: PASSED" >> backend_test_summary.txt
        echo "âœ… Backend API Tests: PASSED" >> backend_test_summary.txt
        echo "âœ… Core Component Tests: PASSED" >> backend_test_summary.txt
        echo "===================" >> backend_test_summary.txt
        echo "ðŸŽ‰ All backend tests completed successfully!" >> backend_test_summary.txt
        echo "Timestamp: $(date)" >> backend_test_summary.txt
        echo "Python Version: $(python --version)" >> backend_test_summary.txt
        cat backend_test_summary.txt

    - name: ðŸ“¤ Upload backend test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: backend-test-results
        path: |
          reports/
          htmlcov/
          test-results.xml
          .coverage
          backend_test_summary.txt
        retention-days: 30

    - name: ðŸ“ˆ Upload coverage to Codecov
      uses: codecov/codecov-action@v4
      if: success()
      with:
        file: ./coverage.xml
        fail_ci_if_error: false
        verbose: true

  # Integration Tests - Medium execution time
  integration-tests:
    name: ðŸ”„ Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: backend-tests

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-ci.txt
        pip install pytest pytest-cov pytest-html

    - name: Run integration tests
      run: |
        python run_tests.py --integration --cov --html=reports/integration-report.html

    - name: Upload integration test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-results
        path: |
          reports/
          htmlcov/
          test-results.xml
        retention-days: 30

  # End-to-End Tests - Longer execution time
  e2e-tests:
    name: ðŸŽ­ End-to-End Tests
    runs-on: ubuntu-latest
    timeout-minutes: 25
    needs: backend-tests

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: 'frontend/package.json'

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-ci.txt
        pip install playwright pytest-playwright pytest-html

    - name: Install frontend dependencies
      run: |
        cd frontend
        rm -rf node_modules package-lock.json || true
        npm install
        npm run build

    - name: Install Playwright browsers
      run: playwright install --with-deps chromium firefox webkit

    - name: Run E2E tests
      env:
        RAPIDAPI_KEY: ${{ secrets.RAPIDAPI_KEY }}
      run: |
        python run_tests.py --e2e --html=reports/e2e-report.html

    - name: Upload E2E test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: e2e-test-results
        path: |
          reports/
          test-results/
          screenshots/
          videos/
        retention-days: 30

    - name: Upload E2E failure artifacts
      uses: actions/upload-artifact@v4
      if: failure()
      with:
        name: e2e-failure-artifacts
        path: |
          screenshots/
          videos/
          logs/
        retention-days: 7

  # Performance Tests - Check for regressions
  performance-tests:
    name: âš¡ Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: backend-tests

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-ci.txt
        pip install pytest pytest-benchmark pytest-html

    - name: Run performance tests
      run: |
        python run_tests.py --performance --html=reports/performance-report.html

    - name: Upload performance test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-test-results
        path: |
          reports/
          .benchmarks/
        retention-days: 30

  # Code Quality and Security Checks
  code-quality:
    name: ðŸ” Code Quality & Security
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install code quality tools
      run: |
        python -m pip install --upgrade pip
        pip install black isort flake8 mypy bandit safety

    - name: Check code formatting with Black
      run: black --check --diff .

    - name: Check import sorting with isort
      run: isort --check-only --diff .

    - name: Lint with flake8
      run: flake8 app tests --max-line-length=100

    - name: Type check with mypy
      run: mypy app --ignore-missing-imports
      continue-on-error: true

    - name: Security check with bandit
      run: bandit -r app -f json -o reports/bandit-report.json
      continue-on-error: true

    - name: Check dependencies for security vulnerabilities
      run: safety check --json --output reports/safety-report.json
      continue-on-error: true

    - name: Upload code quality reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: code-quality-reports
        path: reports/
        retention-days: 30

  # Test Results Summary
  test-summary:
    name: ðŸ“Š Test Results Summary
    runs-on: ubuntu-latest
    needs: [backend-tests, integration-tests, e2e-tests, performance-tests, code-quality]
    if: always()

    steps:
    - name: Download all test artifacts
      uses: actions/download-artifact@v4

    - name: Create test summary
      run: |
        echo "# ðŸ§ª Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Test Suite Results" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸš€ Backend API Tests: ${{ needs.backend-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ”„ Integration Tests: ${{ needs.integration-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸŽ­ End-to-End Tests: ${{ needs.e2e-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- âš¡ Performance Tests: ${{ needs.performance-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- ðŸ” Code Quality: ${{ needs.code-quality.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ "${{ needs.backend-tests.result }}" = "success" ] && [ "${{ needs.integration-tests.result }}" = "success" ]; then
          echo "âœ… **Core testing passed!** All critical tests are working." >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Core testing failed!** Please check the failed test results." >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Test Artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Backend test results and coverage reports" >> $GITHUB_STEP_SUMMARY
        echo "- Integration test results" >> $GITHUB_STEP_SUMMARY
        echo "- E2E test results with screenshots and videos (if available)" >> $GITHUB_STEP_SUMMARY
        echo "- Performance benchmarks" >> $GITHUB_STEP_SUMMARY
        echo "- Code quality and security reports" >> $GITHUB_STEP_SUMMARY
