name: Comprehensive Testing Suite

on:
  push:
    branches: [ main, develop, 'feature/**' ]
  pull_request:
    branches: [ main, develop ]

env:
  PYTHON_VERSION: '3.12'
  NODE_VERSION: '18'

jobs:
  # Backend API Tests - Fast execution
  backend-tests:
    name: 🚀 Backend API Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-ci.txt
        pip install pytest pytest-cov pytest-html pytest-xdist
        
    - name: Run backend API tests
      run: |
        echo "Running CI-friendly backend tests..."
        python run_tests.py --backend
        pytest tests/backend/api/test_backend_fastapi_ci.py -v
        pytest tests/backend/test_user_profiles_pytest.py -v
        
    - name: Upload backend test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: backend-test-results
        path: |
          reports/
          htmlcov/
          test-results.xml
          .coverage
        retention-days: 30
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v4
      if: success()
      with:
        file: ./coverage.xml
        fail_ci_if_error: false
        verbose: true

  # Integration Tests - Medium execution time
  integration-tests:
    name: 🔄 Integration Tests  
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: backend-tests
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-html
        
    - name: Run integration tests
      run: |
        python run_tests.py --integration --cov --html=reports/integration-report.html
        
    - name: Upload integration test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-results
        path: |
          reports/
          htmlcov/
          test-results.xml
        retention-days: 30

  # End-to-End Tests - Longer execution time
  e2e-tests:
    name: 🎭 End-to-End Tests
    runs-on: ubuntu-latest
    timeout-minutes: 25
    needs: backend-tests
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: 'frontend/package-lock.json'
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install playwright pytest-playwright pytest-html
        
    - name: Install frontend dependencies
      run: |
        cd frontend
        npm ci
        npm run build
        
    - name: Install Playwright browsers
      run: playwright install --with-deps chromium firefox webkit
      
    - name: Run E2E tests
      env:
        RAPIDAPI_KEY: ${{ secrets.RAPIDAPI_KEY }}
      run: |
        python run_tests.py --e2e --html=reports/e2e-report.html
        
    - name: Upload E2E test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: e2e-test-results
        path: |
          reports/
          test-results/
          screenshots/
          videos/
        retention-days: 30
        
    - name: Upload E2E failure artifacts
      uses: actions/upload-artifact@v4
      if: failure()
      with:
        name: e2e-failure-artifacts
        path: |
          screenshots/
          videos/
          logs/
        retention-days: 7

  # Performance Tests - Check for regressions
  performance-tests:
    name: ⚡ Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: backend-tests
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-benchmark pytest-html
        
    - name: Run performance tests
      run: |
        python run_tests.py --performance --html=reports/performance-report.html
        
    - name: Upload performance test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-test-results
        path: |
          reports/
          .benchmarks/
        retention-days: 30

  # Code Quality and Security Checks
  code-quality:
    name: 🔍 Code Quality & Security
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install code quality tools
      run: |
        python -m pip install --upgrade pip
        pip install black isort flake8 mypy bandit safety
        
    - name: Check code formatting with Black
      run: black --check --diff .
      
    - name: Check import sorting with isort
      run: isort --check-only --diff .
      
    - name: Lint with flake8
      run: flake8 app tests --max-line-length=100
      
    - name: Type check with mypy
      run: mypy app --ignore-missing-imports
      continue-on-error: true
      
    - name: Security check with bandit
      run: bandit -r app -f json -o reports/bandit-report.json
      continue-on-error: true
      
    - name: Check dependencies for security vulnerabilities
      run: safety check --json --output reports/safety-report.json
      continue-on-error: true
      
    - name: Upload code quality reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: code-quality-reports
        path: reports/
        retention-days: 30

  # Test Results Summary
  test-summary:
    name: 📊 Test Results Summary
    runs-on: ubuntu-latest
    needs: [backend-tests, integration-tests, e2e-tests, performance-tests, code-quality]
    if: always()
    
    steps:
    - name: Download all test artifacts
      uses: actions/download-artifact@v4
      
    - name: Create test summary
      run: |
        echo "# 🧪 Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Test Suite Results" >> $GITHUB_STEP_SUMMARY
        echo "- 🚀 Backend API Tests: ${{ needs.backend-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- 🔄 Integration Tests: ${{ needs.integration-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- 🎭 End-to-End Tests: ${{ needs.e2e-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- ⚡ Performance Tests: ${{ needs.performance-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- 🔍 Code Quality: ${{ needs.code-quality.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.backend-tests.result }}" = "success" ] && [ "${{ needs.integration-tests.result }}" = "success" ]; then
          echo "✅ **Core testing passed!** All critical tests are working." >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **Core testing failed!** Please check the failed test results." >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Test Artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Backend test results and coverage reports" >> $GITHUB_STEP_SUMMARY
        echo "- Integration test results" >> $GITHUB_STEP_SUMMARY
        echo "- E2E test results with screenshots and videos (if available)" >> $GITHUB_STEP_SUMMARY
        echo "- Performance benchmarks" >> $GITHUB_STEP_SUMMARY
        echo "- Code quality and security reports" >> $GITHUB_STEP_SUMMARY

# Workflow-level environment variables
env:
  # Test configuration
  TEST_MODE: true
  PYTEST_CURRENT_TEST: true
  
  # Database configuration for tests
  DATABASE_URL: sqlite:///./test_jobpilot.db
  
  # Disable interactive prompts
  CI: true
  PYTHONUNBUFFERED: 1
  
  # Performance settings
  PYTHONDONTWRITEBYTECODE: 1
  PIP_DISABLE_PIP_VERSION_CHECK: 1
